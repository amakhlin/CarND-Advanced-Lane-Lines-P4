{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_camera_calibration(path='camera_cal/calibration*.jpg', show=False, save_path=None):\n",
    "    '''\n",
    "    Compute and return camera matrix (mtx) and distortion coefficients (dist)\n",
    "        based on checkerboard images stored in location indicated by `path`.\n",
    "        Optional argument `show` displays checherboard images with found corners\n",
    "        highlated for 0.5s. Optional argument `save_path` saves these images for\n",
    "        off-line viewing\n",
    "    '''\n",
    "    \n",
    "    # prepare object points\n",
    "    objp = np.zeros((6*9,3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob(path)\n",
    "\n",
    "\n",
    "    # Step through the list and search for chessboard corners\n",
    "    for i, fname in enumerate(images):\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "           \n",
    "            # Draw and display the corners\n",
    "            img = cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "\n",
    "            if save_path is not None:\n",
    "                cv2.imwrite(save_path + 'chess_corners{}.jpg'.format(i),img)\n",
    "\n",
    "            if show is True:\n",
    "                cv2.imshow('img',img)\n",
    "                cv2.waitKey(500)\n",
    "    \n",
    "    if show is True:\n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1],None,None)\n",
    "        \n",
    "    return mtx, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute camera matrix and distortion coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mtx, dist = get_camera_calibration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distortion Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def undistort_img(img, mtx, dist):\n",
    "    ''' \n",
    "    Return undistorted image based on camera matrix `mtx` and distortion coefficients `dist`\n",
    "    '''\n",
    "    return cv2.undistort(img, mtx, dist, None, mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color and Gradient Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hls_select(img, thresh=(0, 255)):\n",
    "    '''\n",
    "    Return a binary image where pixels which are within the threshold `thresh` of the \n",
    "    S-channel (Saturation) in the HSL colorspace (Hue, Saturation, Lightness) are set\n",
    "    to 255. The rest of the pixels are zero.\n",
    "    '''\n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)\n",
    "    s_channel = hls[:,:,2]\n",
    "    binary_output = np.zeros_like(s_channel)\n",
    "    binary_output[(s_channel > thresh[0]) & (s_channel <= thresh[1])] = 255\n",
    "\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def abs_sobel_thresh(img, orient='x', thresh_min=0, thresh_max=255):\n",
    "    '''\n",
    "    Return a binary image where pixels which are within the threshold `thresh_min`\n",
    "    and `threshold_max` of the sobel gradient along `orient` direction are set to \n",
    "    255. The rest of the pixels are zero.\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply x or y gradient with the OpenCV Sobel() function\n",
    "    # and take the absolute value\n",
    "    if orient == 'x':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 1, 0))\n",
    "    if orient == 'y':\n",
    "        abs_sobel = np.absolute(cv2.Sobel(gray, cv2.CV_64F, 0, 1))\n",
    "    \n",
    "    # Rescale back to 8 bit integer\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    \n",
    "    # Create a copy and apply the threshold\n",
    "    binary_output = np.zeros_like(scaled_sobel)\n",
    "\n",
    "    binary_output[(scaled_sobel >= thresh_min) & (scaled_sobel <= thresh_max)] = 255\n",
    "\n",
    "    # Return the result\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def region_of_interest(img):\n",
    "    \"\"\"\n",
    "    Applies an image mask.\n",
    "    \n",
    "    Only keeps the region of the image defined by the polygon\n",
    "    formed from `vertices`. The rest of the image is set to black.\n",
    "    \"\"\"\n",
    "    \n",
    "    h,w = img.shape[0], img.shape[1]\n",
    "        \n",
    "    vertices = np.array([[[int(0.15*w),int(.95*h)], # bot left\n",
    "                          [int(.45*w),int(.58*h)],  # top left\n",
    "                          [int(.55*w),int(.58*h)],  # top right\n",
    "                          [int(.9*w),int(.95*h)]]],# bot right\n",
    "                            dtype=np.int32 )\n",
    "    \n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros_like(img)   \n",
    "    \n",
    "    #defining a 3 channel or 1 channel color to fill the mask with depending on the input image\n",
    "    if len(img.shape) > 2:\n",
    "        channel_count = img.shape[2]  # i.e. 3 or 4 depending on your image\n",
    "        ignore_mask_color = (255,) * channel_count\n",
    "    else:\n",
    "        ignore_mask_color = 255\n",
    "        \n",
    "    #filling pixels inside the polygon defined by \"vertices\" with the fill color    \n",
    "    cv2.fillPoly(mask, vertices, ignore_mask_color)\n",
    "    \n",
    "    #returning the image only where mask pixels are nonzero\n",
    "    masked_image = cv2.bitwise_and(img, mask)\n",
    "    return masked_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def color_and_grandient_threshold(img):\n",
    "    '''\n",
    "    Perform color and X-gradient thresholding of the input `img` and return a binary\n",
    "    image with the pixels that satisfy either color or grandient threshold set to 255.\n",
    "    '''\n",
    "    s_binary = hls_select(img, thresh=(170,255))\n",
    "    sxbinary = abs_sobel_thresh(img, orient='x', thresh_min=20, thresh_max=150)\n",
    "    combined_binary = np.zeros_like(s_binary)\n",
    "    combined_binary[(s_binary == 255) | (sxbinary == 255)] = 255\n",
    "    out = region_of_interest(combined_binary)\n",
    "    \n",
    "    return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perspective Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_perspective_transform(img_shape):\n",
    "    '''\n",
    "    Compute and return prespective transform matrix `M` based on \n",
    "    coordinates of quadrangle vertices in the source image and corresponding \n",
    "    quadrangle vertices in the destination image. Also compute and return the inverse\n",
    "    transform matrix `Minv`.\n",
    "    '''\n",
    "    \n",
    "    h,w = img_shape[0], img_shape[1]\n",
    "    \n",
    "    top_most = 460\n",
    "    offset = 50\n",
    "    \n",
    "    src = np.float32(\n",
    "        [[579, top_most],\n",
    "         [203, h],\n",
    "         [1127, h],\n",
    "         [704, top_most]])\n",
    "\n",
    "    dst = np.float32(\n",
    "        [[320+offset, 0],\n",
    "         [320+offset, h],\n",
    "         [960-offset, h],\n",
    "         [960-offset, 0]])\n",
    "    \n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    \n",
    "    return M, Minv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute perspective transform and its inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M, Minv = get_perspective_transform((720,1280))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def warp(img, M):\n",
    "    '''\n",
    "    Apply perspective transform to image `img` and return the transformed or warped image\n",
    "    '''\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    return cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Identification and Curve Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_baseline_x(img, lane='left'):\n",
    "    '''\n",
    "    Find the location of the 'hot' pixels at the bottom of the image searching either\n",
    "    left or right half based on parameter `lane`. Return x location of the line.\n",
    "    '''\n",
    "    \n",
    "    # histogram for lower half\n",
    "    histogram = np.sum(img[np.int(img.shape[0]/2):,:], axis=0)\n",
    "    \n",
    "    # find midpoint along x\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    \n",
    "    if lane is 'left':\n",
    "        x_base = np.argmax(histogram[:midpoint])\n",
    "    else:\n",
    "        x_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "    \n",
    "    return x_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sliding_window_search(img, x_base, debug=False, out_img=None):    \n",
    "    '''\n",
    "    Perform sliding window search based on the previously identified starting\n",
    "    location at the bottom of the image `x_base`. The search is performed in \n",
    "    each of several bands along the y axis starting at the bottom in the small\n",
    "    x axis neighborhood of the previously identified x location in the lower window\n",
    "    or the x_base in the case of the bottom most window. Once the pixels in all\n",
    "    windows are identified they are fitted with a 2nd order polynomial whose\n",
    "    coefficients are returned in `fit`. The radius of curvature (ROC) and the \n",
    "    x location of the line are computed using the helper function `roc_and_loc()`\n",
    "    and are also returned. Debugging information is optionally recorded on the \n",
    "    image `out_img` when `debug` flag is set.\n",
    "    '''\n",
    "    \n",
    "    # set parameters\n",
    "    # number of sliding windows (height dir)\n",
    "    nwindows = 9\n",
    "    \n",
    "    # width of the sliding search window is +/- margin\n",
    "    margin = 100\n",
    "    # recenter next window if at least this many points were found\n",
    "    minpix = 50\n",
    "    \n",
    "    #if debug is True:\n",
    "    #    out_img = np.dstack((img, img, img))*255\n",
    "    \n",
    "    # Set height of windows\n",
    "    window_height = np.int(img.shape[0]/nwindows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    # Current positions to be updated for each window\n",
    "    x_current = x_base\n",
    "    \n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y\n",
    "        win_y_low = img.shape[0] - (window+1)*window_height\n",
    "        win_y_high = img.shape[0] - window*window_height\n",
    "        \n",
    "        win_x_low = x_current - margin\n",
    "        win_x_high = x_current + margin\n",
    "        \n",
    "        if debug is True and out_img is not None:\n",
    "            # Draw the windows on the visualization image\n",
    "            cv2.rectangle(out_img,(win_x_low,win_y_low),(win_x_high,win_y_high),(0,255,0), 2)\n",
    "            \n",
    "        \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_inds = ((nonzeroy >= win_y_low) &\n",
    "                     (nonzeroy < win_y_high) & \n",
    "                     (nonzerox >= win_x_low) & \n",
    "                     (nonzerox < win_x_high)).nonzero()[0]\n",
    "        \n",
    "        # Append these indices to the lists\n",
    "        lane_inds.append(good_inds)\n",
    "\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_inds) > minpix:\n",
    "            x_current = np.int(np.mean(nonzerox[good_inds]))\n",
    "\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    lane_inds = np.concatenate(lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    x = nonzerox[lane_inds]\n",
    "    y = nonzeroy[lane_inds] \n",
    "    \n",
    "    if debug is True and out_img is not None:\n",
    "        out_img[nonzeroy[lane_inds], nonzerox[lane_inds]] = [255, 0, 0]\n",
    "\n",
    "\n",
    "    # Fit a second order polynomial\n",
    "    fit = np.polyfit(y, x, 2)\n",
    "    \n",
    "    # find ROC and lane location at bottom of image\n",
    "    r, loc = roc_and_loc(x, y)\n",
    "    \n",
    "    #if debug is True:\n",
    "    #    plt.figure(figsize=(8,10), tight_layout=True)\n",
    "    #    plt.imshow(out_img)\n",
    "\n",
    "    return fit, r, loc, out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def local_fit_search(img, fit, debug=False, out_img=None):\n",
    "    '''\n",
    "    Perform line indentification search along a previously found polynomial `fit`. Return\n",
    "    updated fit as well a radius of curvature and location in the lane.\n",
    "    '''\n",
    "    \n",
    "    # search based on fit from pervious images\n",
    "    \n",
    "    nonzero = img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    \n",
    "    margin = 100\n",
    "    \n",
    "    lane_inds = ((nonzerox > (fit[0]*(nonzeroy**2) + fit[1]*nonzeroy + fit[2] - margin)) &\n",
    "                 (nonzerox < (fit[0]*(nonzeroy**2) + fit[1]*nonzeroy + fit[2] + margin))) \n",
    "\n",
    "    # Extract line pixel positions\n",
    "    x = nonzerox[lane_inds]\n",
    "    y = nonzeroy[lane_inds] \n",
    "    \n",
    "    # Fit a second order polynomial\n",
    "    fit = np.polyfit(y, x, 2)\n",
    "    \n",
    "    if debug is True and out_img is not None:\n",
    "        out_img[nonzeroy[lane_inds], nonzerox[lane_inds]] = [255, 0, 0]\n",
    "        \n",
    "        \n",
    "    # find ROC and lane location at bottom of image\n",
    "    r, loc = roc_and_loc(x, y)\n",
    "\n",
    "    return fit, r, loc, out_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radius of Curvature and Line Location Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def roc_and_loc(x, y):\n",
    "    '''\n",
    "    Compute and return radius of curvature and line location along the x dir\n",
    "    in units of meters\n",
    "    '''\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    \n",
    "    y_eval_m = 720*ym_per_pix\n",
    "    x = x*xm_per_pix\n",
    "    y = y*ym_per_pix\n",
    "    \n",
    "    fit = np.polyfit(y, x, 2)\n",
    "    \n",
    "    r = ((1 + (2.*fit[0]*y_eval_m + fit[1])**2)**1.5) / np.absolute(2*fit[0])\n",
    "    \n",
    "    # evaluate fit at the bottom of image\n",
    "    loc = fit[0]*y_eval_m**2 + fit[1]*y_eval_m + fit[2]\n",
    "    \n",
    "    center_of_image_m = (1280. / 2.) * xm_per_pix\n",
    "    \n",
    "    loc -= center_of_image_m\n",
    "    \n",
    "    return r, loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Frame Qualification and Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_param_filter(line, alpha):\n",
    "    '''\n",
    "    First order filter\n",
    "    '''\n",
    "    line.best_fit = alpha * line.current_fit + (1. - alpha) * line.best_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Class to keep track of line fit parameters from frame to frame\n",
    "'''\n",
    "class Line():\n",
    "    def __init__(self, name):\n",
    "        # name of line 'left' or 'right'\n",
    "        self.name = name\n",
    "        \n",
    "        # car base\n",
    "        self.L = 2.7 # Lincoln MKZ\n",
    "        \n",
    "        # maximum steering magnitude\n",
    "        self.steering_tol = .01 # rad\n",
    "        \n",
    "        # maximumm steerign change from previous frame\n",
    "        self.steering_change_tol = .0016 # rad\n",
    "        \n",
    "        # maximum center change from previous frame\n",
    "        self.centering_tol = 0.2 # m\n",
    "        \n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False \n",
    "        \n",
    "        # disqualifed frame\n",
    "        self.bad_fit = True\n",
    "        \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = [np.array([False])] \n",
    "        \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None\n",
    "        \n",
    "        # redius of curvature of the previous frame\n",
    "        self.radius_of_curvature_prev = None\n",
    "        \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        \n",
    "        # line location of the previous frame\n",
    "        self.line_base_pos_prev = None\n",
    "        \n",
    "        # counter to keep track of disqualifed frames\n",
    "        self.bad_fit_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_search(img, line):\n",
    "    '''\n",
    "    Search for lines in image using either sliding window search or local fit search\n",
    "    depending on weather previous fit is available \n",
    "    '''\n",
    "    if line.detected is False:\n",
    "        x_base = find_baseline_x(img, line.name)\n",
    "        line.current_fit, line.radius_of_curvature, line.line_base_pos, _ = \\\n",
    "            sliding_window_search(img, x_base)\n",
    "        \n",
    "            \n",
    "        line.radius_of_curvature_prev = line.radius_of_curvature\n",
    "        line.line_base_pos_prev = line.line_base_pos\n",
    "        line.best_fit = line.current_fit\n",
    "        \n",
    "    else:\n",
    "        line.current_fit, line.radius_of_curvature, line.line_base_pos, _ = \\\n",
    "            local_fit_search(img, line.best_fit)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def line_qualify(line):\n",
    "    '''\n",
    "    Check if line meets requirements based on steering angle change from previous frame,\n",
    "    absolute steering angle, and line location based change from previous frame.\n",
    "    '''\n",
    "    line.bad_fit = False\n",
    "    \n",
    "    # convert ROC to steering angle\n",
    "    steering = line.L / line.radius_of_curvature\n",
    "    steering_prev = line.L / line.radius_of_curvature_prev\n",
    "    \n",
    "    if np.fabs(steering - steering_prev) > line.steering_change_tol:\n",
    "        line.bad_fit = True\n",
    "        #print(line.name+'steering_change_tol')\n",
    "        \n",
    "    if np.fabs(steering) > line.steering_tol:\n",
    "        line.bad_fit = True\n",
    "        #print(line.name+'steering_tol')\n",
    "        \n",
    "    if np.fabs(line.line_base_pos - line.line_base_pos_prev) > line.centering_tol:\n",
    "        line.bad_fit = True\n",
    "        #print(line.name+'centering_tol')\n",
    "        \n",
    "    if line.bad_fit is not True:\n",
    "        # filter\n",
    "        fit_param_filter(line, 0.20)\n",
    "        \n",
    "        # record previous values\n",
    "        line.radius_of_curvature_prev = line.radius_of_curvature\n",
    "        line_base_pos_prev = line.line_base_pos\n",
    "        line.detected = True\n",
    "        \n",
    "        if line.bad_fit_cnt > 0:\n",
    "            line.bad_fit_cnt -= 1\n",
    "        \n",
    "    else:\n",
    "        line.bad_fit_cnt += 1\n",
    "        \n",
    "    if line.bad_fit_cnt > 20:\n",
    "        line.detected = False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_image(img, line_l, line_r):\n",
    "    '''\n",
    "    Implement the processing pipeline, process a raw input video frame image and return\n",
    "    an image with the line markings and ROC and LOC data displayed.\n",
    "    '''\n",
    "    \n",
    "    # distortion correction\n",
    "    img = undistort_img(img, mtx, dist)\n",
    "    \n",
    "    # color and gradient thresholding\n",
    "    img_th = color_and_grandient_threshold(img)\n",
    "    \n",
    "    # perspective transform\n",
    "    img_th = warp(img_th, M)\n",
    "    \n",
    "    # line identification\n",
    "    line_search(img_th, line_l)\n",
    "    line_search(img_th, line_r)\n",
    "    \n",
    "    # line qualification\n",
    "    line_qualify(line_l)\n",
    "    line_qualify(line_r)\n",
    "    \n",
    "    #loc_error = loc_left + loc_right\n",
    "    \n",
    "    # line drawing and perspetive inverse transform\n",
    "    out = draw_overlay(img, line_l.best_fit, line_r.best_fit, Minv)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Display of the Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_overlay(img, fit_l, fit_r, Minv):\n",
    "    '''\n",
    "    Display line markings and radius of curvature and lane centering information\n",
    "    on image `img` using fit data `fit_l` and `fit_r` for left and right lines \n",
    "    respectively. Unwarp the image back to camera space using `Minv` perspective\n",
    "    transformation matrix.\n",
    "    '''\n",
    "    \n",
    "    # Create an image to draw the lines on\n",
    "    color_warp = np.zeros_like(img).astype(np.uint8)\n",
    "    #color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "    ploty = np.linspace(0, 719, num=720)# to cover same y-range as image\n",
    "    left_fitx = fit_l[0]*ploty**2 + fit_l[1]*ploty + fit_l[2]\n",
    "    right_fitx = fit_r[0]*ploty**2 + fit_r[1]*ploty + fit_r[2]\n",
    "    \n",
    "    # recompute ROC and LOC based on filtered fit\n",
    "    r_left, loc_l = roc_and_loc(left_fitx, ploty)\n",
    "    r_right, loc_r = roc_and_loc(right_fitx, ploty)\n",
    "    loc_error = loc_l + loc_r\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (img.shape[1], img.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(img, 1, newwarp, 0.3, 0)\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    cv2.putText(result, '{:s}: {:>+5.2f} m'.format('lane centering', loc_error), (450,700), font, 1,(255,255,255),2)\n",
    "    cv2.putText(result, '{:s}: {:+.2f} m'.format('ROC', r_left), (50,700), font, 1,(255,255,255),2)\n",
    "    cv2.putText(result, '{:s}: {:+.2f} m'.format('ROC', r_right), (970,700), font, 1,(255,255,255),2)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_image_processor():\n",
    "    '''\n",
    "    Create an video image processor that can be passed to moviepy \n",
    "    '''\n",
    "    line_l = Line('left')\n",
    "    line_r = Line('right')\n",
    "    \n",
    "    def image_processor(img):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        out = process_image(img, line_l, line_r)\n",
    "        return cv2.cvtColor(out, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    return image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proc = make_image_processor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_video_try.mp4\n",
      "[MoviePy] Writing video output_video_try.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [02:57<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_video_try.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_video = 'output_video_try.mp4'\n",
    "clip = VideoFileClip(\"project_video.mp4\")\n",
    "out_clip = clip.fl_image(proc)\n",
    "out_clip.write_videofile(output_video, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
